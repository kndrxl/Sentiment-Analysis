{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm \n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from scipy.special import softmax\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "MODEL_PATH = f\"../models/{MODEL_NAME}\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    new_text = []\n",
    " \n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        # t = '' if t.startswith(('@', 'http')) and len(t) > 1 else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "def infer(text):\n",
    "    labels = ['negative', 'neutral', 'positive']\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    output = ranking[0]\n",
    "    label = labels[output]\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(predictions, true_labels):\n",
    "    cls_report = classification_report(\n",
    "        true_labels, \n",
    "        predictions\n",
    "    )\n",
    "    cnf_matrix = confusion_matrix(\n",
    "        true_labels, \n",
    "        predictions\n",
    "    )\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    print(\"Classification Report:\\n\", cls_report)\n",
    "    print(\"Confusion Matrix:\\n\", cnf_matrix)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "\n",
    "\n",
    "def test_batch(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    data = df['text'].tolist()\n",
    "    true_labels = df['expected_sentiment'].tolist()\n",
    "    predictions = []\n",
    "\n",
    "    for i in tqdm(range(len(data)), desc=\"Processing\"):\n",
    "        text = data[i]\n",
    "        true_label = true_labels[i]\n",
    "        processed_text, predicted_label = infer(text)\n",
    "        predictions.append(predicted_label)\n",
    "\n",
    "    evaluate(predictions, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 498/498 [00:42<00:00, 11.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.82      0.88       177\n",
      "     neutral       0.83      0.86      0.85       139\n",
      "    positive       0.84      0.91      0.88       182\n",
      "\n",
      "    accuracy                           0.87       498\n",
      "   macro avg       0.87      0.87      0.87       498\n",
      "weighted avg       0.87      0.87      0.87       498\n",
      "\n",
      "Confusion Matrix:\n",
      " [[146  14  17]\n",
      " [  5 120  14]\n",
      " [  5  11 166]]\n",
      "Accuracy:  0.8674698795180723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "csv_path = '../data/sentiment_test_cases.csv'\n",
    "test_batch(csv_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
